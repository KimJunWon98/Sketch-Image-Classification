{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af8a2c0-45fe-4d13-bebd-0dca87a7b71f",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c611c8-2226-433c-bf5f-343cc0b094af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/min000914/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 필요 library들을 import합니다.\n",
    "import os\n",
    "from typing import Tuple, Any, Callable, List, Optional, Union\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d69e6a-a719-4a97-92ca-6354c873313f",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f97229-e29f-479d-abab-0db8219d1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root_dir: str, \n",
    "        info_df: pd.DataFrame, \n",
    "        transform: Callable,\n",
    "        is_inference: bool = False\n",
    "    ):\n",
    "        # 데이터셋의 기본 경로, 이미지 변환 방법, 이미지 경로 및 레이블을 초기화합니다.\n",
    "        self.root_dir = root_dir  # 이미지 파일들이 저장된 기본 디렉토리\n",
    "        self.transform = transform  # 이미지에 적용될 변환 처리\n",
    "        self.is_inference = is_inference # 추론인지 확인\n",
    "        self.image_paths = info_df['image_path'].tolist()  # 이미지 파일 경로 목록\n",
    "        \n",
    "        if not self.is_inference:\n",
    "            self.targets = info_df['target'].tolist()  # 각 이미지에 대한 레이블 목록\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # 데이터셋의 총 이미지 수를 반환합니다.\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[Tuple[torch.Tensor, int], torch.Tensor]:\n",
    "        # 주어진 인덱스에 해당하는 이미지를 로드하고 변환을 적용한 후, 이미지와 레이블을 반환합니다.\n",
    "        img_path = os.path.join(self.root_dir, self.image_paths[index])  # 이미지 경로 조합\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)  # 이미지를 BGR 컬러 포맷의 numpy array로 읽어옵니다.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR 포맷을 RGB 포맷으로 변환합니다.\n",
    "        image = self.transform(image)  # 설정된 이미지 변환을 적용합니다.\n",
    "\n",
    "        if self.is_inference:\n",
    "            return image\n",
    "        else:\n",
    "            target = self.targets[index]  # 해당 이미지의 레이블\n",
    "            return image, target  # 변환된 이미지와 레이블을 튜플 형태로 반환합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07d2d0-9585-45ce-8ece-4f69b98f6dd4",
   "metadata": {},
   "source": [
    "# Transform Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a683988-0f73-4e43-907b-0d5209550abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, is_train: bool = True,max_size: int = 288):\n",
    "        self.max_size=max_size\n",
    "        # 공통 변환 설정: 이미지 리사이즈, 정규화, 텐서 변환\n",
    "        common_transforms = [\n",
    "            A.Resize(self.max_size, self.max_size),  # 이미지를 224x224 크기로 리사이즈\n",
    "            #A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 정규화\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()  # albumentations에서 제공하는 PyTorch 텐서 변환\n",
    "        ]\n",
    "        if is_train:\n",
    "            # 훈련용 변환: 랜덤 수평 뒤집기, 랜덤 회전, 랜덤 밝기 및 대비 조정 추가\n",
    "            dropout_transform = A.CoarseDropout(\n",
    "                max_holes=15, \n",
    "                max_height=int(0.1 * max_size), \n",
    "                max_width=int(0.1 * max_size), \n",
    "                fill_value=[random.randint(0, 127)] * 3,  # 0(검정)~128(회색) 사이 무작위 값을 선택하여 RGB 동일하게 적용\n",
    "                p=0.5\n",
    "            )\n",
    "            # OpenCV 기반의 Erosion 및 Dilation 함수 정의\n",
    "            def apply_erosion(img, **params):\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "                return cv2.erode(img, kernel, iterations=1)\n",
    "\n",
    "            def apply_dilation(img, **params):\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "                return cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "            # 훈련용 변환: 랜덤 수평 뒤집기, 랜덤 회전, 랜덤 밝기 및 대비 조정 추가\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    # Geometric transformations\n",
    "                    A.Rotate(limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=255),  # 빈 공간을 흰색(255)으로 채움\n",
    "                    A.HorizontalFlip(p=0.5),  # 50% 확률로 이미지를 수평 뒤집기\n",
    "                    A.VerticalFlip(p=0.2),\n",
    "                    A.Affine(scale=(0.8, 1.2), shear=(-10, 10), p=0.5, border_mode=cv2.BORDER_CONSTANT, cval=255),  # 빈 공간을 흰색(255)으로 채움\n",
    "                    A.ElasticTransform(alpha=1, sigma=10, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=255),  # 빈 공간을 흰색(255)으로 채움\n",
    "\n",
    "                    #dropout_transform,\n",
    "                    A.Lambda(image=self.add_random_text, p=0.3),\n",
    "                    A.Lambda(image=apply_dilation,p=0.4),\n",
    "                    A.Lambda(image=apply_erosion,p=0.4),\n",
    "\n",
    "                    # Noise and blur\n",
    "                    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "                    A.MotionBlur(blur_limit=(3, 7), p=0.5),\n",
    "\n",
    "                    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),\n",
    "\n",
    "                    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "                    A.RandomBrightnessContrast(p=0.2),\n",
    "                ] + common_transforms\n",
    "            )\n",
    "        else:\n",
    "            # 검증/테스트용 변환: 공통 변환만 적용\n",
    "            self.transform = A.Compose(common_transforms)\n",
    "    \n",
    "    def add_random_text(self, image: np.ndarray, **kwargs) -> np.ndarray: #증강용 함수, 이미지에 랜덤 텍스트를 추가한다.\n",
    "        def random_word():\n",
    "            letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "            length = random.randint(3, 15)  # 4에서 12 사이의 길이로 랜덤 설정\n",
    "            return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "        # 임의의 투명도 생성 함수\n",
    "        def random_alpha():\n",
    "            return random.uniform(0.5, 1.0)  # 0.3은 거의 투명, 1.0은 불투명\n",
    "\n",
    "        # 임의의 폰트 크기 생성 함수\n",
    "        def random_font_size():\n",
    "            return random.uniform(0.3, 2)  # OpenCV에서는 폰트 크기를 스케일로 조정\n",
    "\n",
    "        # 이미지 크기\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # 텍스트를 그릴 위치들 (상단 및 하단에서만 랜덤 좌표 선택)\n",
    "        top_y = random.randint(10, height // 5)  # 상단 영역의 랜덤 Y 좌표\n",
    "        bottom_y = random.randint(height - height // 5, height - 10)  # 하단 영역의 랜덤 Y 좌표\n",
    "        random_y = random.choice([top_y, bottom_y])\n",
    "\n",
    "        # X 좌표는 이미지 폭에 따라 랜덤 설정\n",
    "        random_x = random.randint(10, width - 100)\n",
    "\n",
    "        # OpenCV 폰트 설정\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "\n",
    "        # 랜덤 폰트, 색상, 투명도, 텍스트 추가\n",
    "        random_text = random_word()\n",
    "        color = (0,0,0)\n",
    "        font_scale = random_font_size()  # 폰트 크기\n",
    "        alpha = random_alpha()  # 투명도\n",
    "        \n",
    "        # 투명도를 적용한 텍스트 이미지 생성\n",
    "        overlay = image.copy()\n",
    "        cv2.putText(overlay, random_text, (random_x, random_y), font, font_scale, color, thickness=2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "        # 알파 블렌딩으로 텍스트 투명도 조절\n",
    "        image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)    \n",
    "        return image\n",
    "    \n",
    "    def image_resize_with_padding(self, image): #이미지를 종횡비가 깨지지 않게, max_size*max_size로 Resize 및 Padding한다.\n",
    "        h, w = image.shape[:2]\n",
    "        if w > h:\n",
    "            new_w = self.max_size\n",
    "            new_h = int(h * (self.max_size / w))\n",
    "        else:\n",
    "            new_h = self.max_size\n",
    "            new_w = int(w * (self.max_size / h))\n",
    "    \n",
    "        resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        # 패딩 추가\n",
    "        top = (self.max_size - new_h) // 2\n",
    "        bottom = self.max_size - new_h - top\n",
    "        left = (self.max_size - new_w) // 2\n",
    "        right = self.max_size - new_w - left\n",
    "        \n",
    "        padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
    "        return padded_image\n",
    "    \n",
    "    def blackBackgorund_to_whiteBackground(self, image): #검은색 배경의 이미지라면 하얀색 배경의 이미지로 바꾼다.\n",
    "        if np.mean(image) <= 127:\n",
    "            image = 255 - image\n",
    "        return image\n",
    "    \n",
    "    def enhance_and_binarize(self, image: np.ndarray, canny_threshold1: int = 100, canny_threshold2: int = 200, weight: float = 0.8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        그레이스케일 이미지에 대해 평균값을 기반으로 픽셀을 강조하고\n",
    "        Canny Edge를 적용하여 선을 강조한 뒤 다시 평균값을 기반으로 픽셀을 강조한다..\n",
    "        \"\"\"\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # 픽셀의 평균값 계산 후 평균보다 큰 픽셀들을 255로 설정\n",
    "        mean_value = np.mean(gray_image)\n",
    "        gray_image[gray_image > mean_value] = 255\n",
    "\n",
    "        # Canny Edge를 이용해 선 강조\n",
    "        edges = cv2.Canny(gray_image, threshold1=canny_threshold1, threshold2=canny_threshold2)\n",
    "        enhanced_image = cv2.addWeighted(gray_image, weight, edges, 1 - weight, 0)\n",
    "\n",
    "        # 픽셀의 평균값 계산 후 평균보다 큰 픽셀들을 255로 설정\n",
    "        mean_value = np.mean(enhanced_image)\n",
    "        enhanced_image[enhanced_image > mean_value] = 255\n",
    "\n",
    "        return enhanced_image\n",
    "\n",
    "\n",
    "    def __call__(self, image) -> torch.Tensor:\n",
    "        # 이미지가 NumPy 배열인지 확인\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            raise TypeError(\"Image should be a NumPy array (OpenCV format).\")\n",
    "        channel_diff = np.max(image, axis=-1) - np.min(image, axis=-1)\n",
    "        if np.mean(channel_diff) > 50:  # 차이 값이 작으면 무채색이 많다고 판단\n",
    "            image=self.image_resize_with_padding(image=image)\n",
    "            transformed = self.transform(image=image)  # 이미지에 설정된 변환을 적용\n",
    "            return transformed['image']\n",
    "        \n",
    "        image = self.blackBackgorund_to_whiteBackground(image)\n",
    "        \n",
    "        image=self.enhance_and_binarize(image=image)\n",
    "        image=self.image_resize_with_padding(image=image)\n",
    "\n",
    "        graytorgb = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "        transformed = self.transform(image=graytorgb)  # 이미지에 설정된 변환을 적용\n",
    "        \n",
    "        return transformed['image']  # 변환된 이미지의 텐서를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82f3416-86f2-430f-9260-d23904e757e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformSelector:\n",
    "    \"\"\"\n",
    "    이미지 변환 라이브러리를 선택하기 위한 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform_type: str, max_size:int):\n",
    "        self.max_size=max_size\n",
    "        # 지원하는 변환 라이브러리인지 확인\n",
    "        if transform_type in [\"torchvision\", \"albumentations\"]:\n",
    "            self.transform_type = transform_type\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown transformation library specified.\")\n",
    "\n",
    "    def get_transform(self, is_train: bool):\n",
    "        if self.transform_type == 'albumentations':\n",
    "            transform = AlbumentationsTransform(is_train=is_train,max_size=self.max_size)\n",
    "        \n",
    "        return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938bcb2-9257-49cb-8d05-dd4a7bb25665",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28c8e4f-a914-4b12-982e-d4a58863c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Timm 라이브러리를 사용하여 다양한 사전 훈련된 모델을 제공하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        num_classes: int, \n",
    "        pretrained: bool\n",
    "    ):\n",
    "        super(TimmModel, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2da081-9010-431d-a049-835d7bbea4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelector:\n",
    "    \"\"\"\n",
    "    사용할 모델 유형을 선택하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_type: str, \n",
    "        num_classes: int, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        if model_type == 'timm':\n",
    "            self.model = TimmModel(num_classes=num_classes, **kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type specified.\")\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "\n",
    "        # 생성된 모델 객체 반환\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2977c7b-bc39-48f7-8155-ef6b6a03d6f8",
   "metadata": {},
   "source": [
    "# Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97471eb3-a979-4fb3-b976-6c3177c79f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    모델의 손실함수를 계산하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(label_smoothing=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: torch.Tensor, \n",
    "        targets: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "        return self.loss_fn(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e3d21-5ab8-41b0-aa5c-e62ace8dc6a6",
   "metadata": {},
   "source": [
    "# Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a90c673-6672-4066-a9ec-9975d7842be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        device: torch.device, \n",
    "        train_loader: DataLoader, \n",
    "        val_loader: DataLoader, \n",
    "        optimizer: optim.Optimizer,\n",
    "        scheduler: optim.lr_scheduler,\n",
    "        loss_fn: torch.nn.modules.loss._Loss, \n",
    "        epochs: int,\n",
    "        result_path: str\n",
    "    ):\n",
    "        # 클래스 초기화: 모델, 디바이스, 데이터 로더 등 설정\n",
    "        self.model = model  # 훈련할 모델\n",
    "        self.device = device  # 연산을 수행할 디바이스 (CPU or GPU)\n",
    "        self.train_loader = train_loader  # 훈련 데이터 로더\n",
    "        self.val_loader = val_loader  # 검증 데이터 로더\n",
    "        self.optimizer = optimizer  # 최적화 알고리즘\n",
    "        self.scheduler = scheduler # 학습률 스케줄러\n",
    "        self.loss_fn = loss_fn  # 손실 함수\n",
    "        self.epochs = epochs  # 총 훈련 에폭 수\n",
    "        self.result_path = result_path  # 모델 저장 경로\n",
    "        self.best_models = [] # 가장 좋은 상위 3개 모델의 정보를 저장할 리스트\n",
    "        self.lowest_loss = float('inf') # 가장 낮은 Loss를 저장할 변수\n",
    "        self.patience=14\n",
    "        self.early_stopping_counter = 0 \n",
    "\n",
    "    def save_model(self, epoch, loss, fold):\n",
    "        # 모델 저장 경로 설정\n",
    "        os.makedirs(self.result_path, exist_ok=True)\n",
    "\n",
    "        # 현재 에폭 모델 저장\n",
    "        current_model_path = os.path.join(self.result_path, f'fold_{fold}_epoch_{epoch}_loss_{loss:.4f}.pt')\n",
    "        torch.save(self.model.state_dict(), current_model_path)\n",
    "\n",
    "        # 최상위 3개 모델 관리\n",
    "        self.best_models.append((loss, epoch, current_model_path))\n",
    "        self.best_models.sort()\n",
    "        if len(self.best_models) > 3:\n",
    "            _, _, path_to_remove = self.best_models.pop(-1)  # 가장 높은 손실 모델 삭제\n",
    "            if os.path.exists(path_to_remove):\n",
    "                os.remove(path_to_remove)\n",
    "\n",
    "        # 가장 낮은 손실의 모델 저장\n",
    "        if loss <= self.lowest_loss:\n",
    "            self.lowest_loss = loss\n",
    "            self.early_stopping_counter = 0  # 손실이 개선된 경우, 카운터 리셋\n",
    "            print(f\"Improvement in validation loss. Reset early stopping counter.\")\n",
    "            best_model_path = os.path.join(self.result_path, f'fold_{fold}_best_model.pt')\n",
    "            torch.save(self.model.state_dict(), best_model_path)\n",
    "            print(f\"Save {epoch} epoch result. Loss = {loss:.4f}\")\n",
    "        else:\n",
    "            self.early_stopping_counter += 1  # 손실이 개선되지 않은 경우, 카운터 증가\n",
    "            print(f\"No improvement in validation loss. Early stopping counter: {self.early_stopping_counter}/{self.patience}\")\n",
    "            \n",
    "    def train_epoch(self) -> float:\n",
    "        # 한 에폭 동안의 훈련을 진행\n",
    "        self.model.train()\n",
    "        scaler = GradScaler() \n",
    "        \n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for images, targets in progress_bar:\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "            \n",
    "            # GradScaler로 backward 및 optimizer step\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            #self.scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def validate(self) -> float:\n",
    "        # 모델의 검증을 진행\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(self.val_loader, desc=\"Validating\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in progress_bar:\n",
    "                images, targets = images.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(images)    \n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                #self.scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        return total_loss / len(self.val_loader)\n",
    "\n",
    "    def train(self, fold) -> None:\n",
    "        # 전체 훈련 과정을 관리\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current Learning Rate: {current_lr:.9f}\")\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss = self.validate()\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\\n\")\n",
    "            \n",
    "            self.save_model(epoch, val_loss, fold)\n",
    "            \n",
    "            # Early Stopping 조건을 만족하면 중단\n",
    "            if self.early_stopping_counter >= self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "            self.scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f41c09-318a-4f2e-bdca-68d8a07e9938",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698783c4-ac2a-4e66-82aa-637df06ce012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 장비를 선택.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 학습 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
    "traindata_dir = \"/home/min000914/cv_17_last/data/train\"\n",
    "traindata_info_file = \"/home/min000914/cv_17_last/data/train.csv\"\n",
    "save_result_path = \"/home/min000914/cv_17_last/model_output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "040977a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109716/3266876058.py:38: UserWarning: Argument 'border_mode' is not valid and will be ignored.\n",
      "  A.Affine(scale=(0.8, 1.2), shear=(-10, 10), p=0.5, border_mode=cv2.BORDER_CONSTANT, cval=255),  # 빈 공간을 흰색(255)으로 채움\n",
      "/home/min000914/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 @@@@@@@@@@@@@@\n",
      "Training fold 1/2\n",
      "Epoch 1/2\n",
      "Current Learning Rate: 0.000012000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 126\u001b[0m\n\u001b[1;32m    113\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    114\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m    115\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m     result_path\u001b[38;5;241m=\u001b[39msave_result_path\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 110\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, fold)\u001b[0m\n\u001b[1;32m    108\u001b[0m current_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Learning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.9f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 77\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# GradScaler로 backward 및 optimizer step\u001b[39;00m\n\u001b[1;32m     76\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 77\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#self.scheduler.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
    "ENSEMBLE_MODELS = [\"convnextv2_huge\" for _ in range(2)]\n",
    "# 교차 검증을 위한 StratifiedKFold 설정\n",
    "transform_selector = TransformSelector(\n",
    "    transform_type = \"albumentations\",\n",
    "    max_size=288\n",
    ")\n",
    "train_transform = transform_selector.get_transform(is_train=True)\n",
    "val_transform = transform_selector.get_transform(is_train=False)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=len(ENSEMBLE_MODELS), shuffle=True, random_state=42)  # 5개의 폴드로 나눔\n",
    "\n",
    "# 학습 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
    "train_info = pd.read_csv(traindata_info_file)\n",
    "\n",
    "# 총 class의 수를 측정.\n",
    "num_classes = len(train_info['target'].unique())\n",
    "\n",
    "fold_range = [0, 1] #다른 서버에서 병렬로 돌리기 위해 fold 분리\n",
    "\n",
    "# StratifiedKFold로 데이터를 나누고 교차 검증을 수행\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_info, train_info['target'])):\n",
    "    if fold in fold_range:\n",
    "        # fold마다 학습이 끝난 후 GPU 메모리 비우기\n",
    "        torch.cuda.empty_cache()\n",
    "        print(fold,\"@@@@@@@@@@@@@@\")\n",
    "        # 랜덤 시드 생성 및 설정\n",
    "        random_seed = random.randint(0, 10000)  # 0부터 10000 사이의 랜덤한 시드 생성\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "        print(f\"Training fold {fold+1}/{skf.get_n_splits()}\")\n",
    "\n",
    "        train_df = train_info.iloc[train_idx]\n",
    "        val_df = train_info.iloc[val_idx]\n",
    "\n",
    "        # 학습에 사용할 Dataset을 선언.\n",
    "        train_dataset1 = CustomDataset(\n",
    "            root_dir=traindata_dir,\n",
    "            info_df=train_df,\n",
    "            transform=train_transform\n",
    "        )\n",
    "        train_dataset2 = CustomDataset(\n",
    "            root_dir=traindata_dir,\n",
    "            info_df=train_df,\n",
    "            transform=val_transform\n",
    "        )\n",
    "        train_dataset=train_dataset1+train_dataset2\n",
    "        \n",
    "        val_dataset1 = CustomDataset(\n",
    "            root_dir=traindata_dir,\n",
    "            info_df=val_df,\n",
    "            transform=train_transform\n",
    "        )\n",
    "        val_dataset2 = CustomDataset(\n",
    "            root_dir=traindata_dir,\n",
    "            info_df=val_df,\n",
    "            transform=val_transform\n",
    "        )\n",
    "        val_dataset=val_dataset1+val_dataset2\n",
    "\n",
    "        # 학습에 사용할 DataLoader를 선언.\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=32, \n",
    "            shuffle=True,\n",
    "            num_workers=16\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=32, \n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        # 모델 초기화\n",
    "        model_selector = ModelSelector(\n",
    "        model_type='timm', \n",
    "        num_classes=num_classes,\n",
    "        model_name='convnext_tiny', \n",
    "        pretrained=True\n",
    "        )\n",
    "        model = model_selector.get_model()\n",
    "\n",
    "        # 모델을 장치로 이동\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.000012,weight_decay=0.0003)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.7)\n",
    "        '''scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
    "                                                  first_cycle_steps=10,\n",
    "                                                  cycle_mult=0.7,\n",
    "                                                  max_lr=0.000135,\n",
    "                                                  min_lr=0.00001,\n",
    "                                                  warmup_steps=4,\n",
    "                                                  gamma=0.6)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                    mode='min',        # 최소화할 지표 (보통 'val_loss' 기준으로)\n",
    "                                                    factor=0.3,        # 학습률을 감소시키는 비율 (0.1 = 10%로 감소)\n",
    "                                                    patience=4,        # 5 에폭 동안 성능 개선이 없으면 학습률 감소\n",
    "                                                    verbose=True)        # 학습률이 줄어들 때 출력                                     \n",
    "        '''\n",
    "        \n",
    "        loss_fn = Loss()\n",
    "\n",
    "        # Trainer 설정\n",
    "        trainer = Trainer(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader, \n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            loss_fn=loss_fn, \n",
    "            epochs=2,\n",
    "            result_path=save_result_path\n",
    "        )\n",
    "\n",
    "        # 모델 학습\n",
    "        trainer.train(fold=fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1dcba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11087088-9b1f-4f7d-8eb5-72008cc88a50",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877eab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def weighted_ensemble_inference_with_augmentation(\n",
    "    models: List[nn.Module],  # 여러 개의 모델 리스트\n",
    "    device: torch.device, \n",
    "    test_loader: DataLoader,  # 두 데이터의 순서가 동일해야함. shuffle=False\n",
    "    test_aug_loader: DataLoader,  # 증강된 데이터 로더\n",
    "    num_classes: int\n",
    "):\n",
    "    num_samples = len(test_loader.dataset)\n",
    "    predictions = np.zeros((num_samples, num_classes))\n",
    "    \n",
    "    # 각 모델별 예측 확률을 저장할 리스트\n",
    "    model_outputs_original = []\n",
    "    model_outputs_augmented = []\n",
    "\n",
    "    # 모든 모델에 대해 예측 (원본 데이터와 증강 데이터 각각 수행)\n",
    "    for model_idx, model in enumerate(models):\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        fold_predictions_original = []\n",
    "        fold_predictions_augmented = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # tqdm에 설명을 추가하여 진행 상황을 각각 표시\n",
    "            for batch_idx, (images, images_aug) in enumerate(tqdm(zip(test_loader, test_aug_loader), desc=f\"Model {model_idx+1} Inference\", total=len(test_loader))):\n",
    "                images = images.to(device)\n",
    "                images_aug = images_aug.to(device)\n",
    "\n",
    "                # 원본 데이터에 대한 예측\n",
    "                logits_original = model(images)\n",
    "                probs_original = F.softmax(logits_original, dim=1)\n",
    "                fold_predictions_original.append(probs_original.cpu().numpy())\n",
    "\n",
    "                # 증강 데이터에 대한 예측\n",
    "                logits_augmented = model(images_aug)\n",
    "                probs_augmented = F.softmax(logits_augmented, dim=1)\n",
    "                fold_predictions_augmented.append(probs_augmented.cpu().numpy())\n",
    "\n",
    "        # 모델별 확률 저장\n",
    "        model_output_original = np.vstack(fold_predictions_original)\n",
    "        model_output_augmented = np.vstack(fold_predictions_augmented)\n",
    "        model_outputs_original.append(model_output_original)\n",
    "        model_outputs_augmented.append(model_output_augmented)\n",
    "\n",
    "    # 각 데이터별로 가중치를 적용하여 앙상블 예측 (원본 vs 증강)\n",
    "    for i in range(num_samples):\n",
    "        # 각 데이터에 대해 모델별 확률 최대값 계산 (원본과 증강 각각)\n",
    "        sample_confidences_original = [np.max(model_output[i]) for model_output in model_outputs_original]\n",
    "        sample_confidences_augmented = [np.max(model_output[i]) for model_output in model_outputs_augmented]\n",
    "\n",
    "        # 원본과 증강의 전체 확신도 합산\n",
    "        total_confidence_original = sum(sample_confidences_original)\n",
    "        total_confidence_augmented = sum(sample_confidences_augmented)\n",
    "        \n",
    "        # 원본 데이터와 증강 데이터에 대한 가중치 계산\n",
    "        weight_original = total_confidence_original / (total_confidence_original + total_confidence_augmented)\n",
    "        weight_augmented = total_confidence_augmented / (total_confidence_original + total_confidence_augmented)\n",
    "        \n",
    "        # 각 모델의 예측값에 가중치를 곱하여 합산\n",
    "        for model_output_original, model_output_augmented in zip(model_outputs_original, model_outputs_augmented):\n",
    "            predictions[i] += model_output_original[i] * weight_original\n",
    "            predictions[i] += model_output_augmented[i] * weight_augmented\n",
    "\n",
    "    # 최종 예측값 반환 (가장 높은 확률을 가진 클래스를 선택)\n",
    "    return np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb89c12-3b5d-4647-a8c2-83650dce6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
    "testdata_dir = \"/home/min000914/cv_17_last/data/test\"\n",
    "testdata_info_file = \"/home/min000914/cv_17_last/data/test.csv\"\n",
    "save_result_path = \"/home/min000914/cv_17_last/model_output\"\n",
    "\n",
    "# 추론 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
    "test_info = pd.read_csv(testdata_info_file)\n",
    "\n",
    "# 총 class 수.\n",
    "num_classes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecec8773-6045-401e-b307-0a9758374c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116667/3266876058.py:38: UserWarning: Argument 'border_mode' is not valid and will be ignored.\n",
      "  A.Affine(scale=(0.8, 1.2), shear=(-10, 10), p=0.5, border_mode=cv2.BORDER_CONSTANT, cval=255),  # 빈 공간을 흰색(255)으로 채움\n"
     ]
    }
   ],
   "source": [
    "# 추론에 사용할 Transform을 선언.\n",
    "transform_selector = TransformSelector(\n",
    "    transform_type = \"albumentations\",\n",
    "    max_size=288\n",
    ")\n",
    "test_transform = transform_selector.get_transform(is_train=False)\n",
    "test__aug_transform = transform_selector.get_transform(is_train=True)\n",
    "\n",
    "# 추론에 사용할 Dataset을 선언.\n",
    "test_dataset = CustomDataset(\n",
    "    root_dir=testdata_dir,\n",
    "    info_df=test_info,\n",
    "    transform=test_transform,\n",
    "    is_inference=True\n",
    ")\n",
    "# 추론에 사용할 증강 Dataset을 선언.\n",
    "test__aug_dataset = CustomDataset(\n",
    "    root_dir=testdata_dir,\n",
    "    info_df=test_info,\n",
    "    transform=test__aug_transform,\n",
    "    is_inference=True\n",
    ")\n",
    "\n",
    "# 추론에 사용할 DataLoader를 선언.\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# 추론에 사용할 증강 DataLoader를 선언.\n",
    "test_aug_loader = DataLoader(\n",
    "    test__aug_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99cc06c1-ce65-476b-8d5b-b8025fcde443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론에 사용할 장비를 선택.\n",
    "# torch라이브러리에서 gpu를 인식할 경우, cuda로 설정.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 추론에 사용할 Model을 선언.\n",
    "model_selector = ModelSelector(\n",
    "    model_type='timm', \n",
    "    num_classes=num_classes,\n",
    "    model_name='convnext_tiny', \n",
    "    pretrained=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317b966a-254c-4ac6-b9b4-1d39f59d524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개의 모델 경로 설정 (예시로 best_model1.pt, best_model2.pt, ... )\n",
    "model_paths = [\n",
    "    os.path.join(save_result_path, \"fold_0_best_model.pt\"),\n",
    "    os.path.join(save_result_path, \"fold_1_best_model.pt\"),\n",
    "]\n",
    "\n",
    "# 모델을 불러와 리스트에 저장\n",
    "models = []\n",
    "for model_path in model_paths:\n",
    "    model = model_selector.get_model()  # 모델 초기화\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2f8e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 1 Inference: 100%|██████████| 313/313 [07:11<00:00,  1.38s/it]\n",
      "Model 2 Inference: 100%|██████████| 313/313 [07:11<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 추론 수행\n",
    "predictions = weighted_ensemble_inference_with_augmentation(\n",
    "    models=models,  # 모델 리스트 전달\n",
    "    device=device, \n",
    "    test_loader=test_loader,\n",
    "    test_aug_loader=test_aug_loader,  # 증강된 데이터 로더도 전달\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc96c889-2423-42b2-8c3c-4b1d364ece71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 클래스에 대한 예측 결과를 하나의 문자열로 합침\n",
    "test_info['target'] = predictions\n",
    "test_info = test_info.reset_index().rename(columns={\"index\": \"ID\"})\n",
    "# DataFrame 저장\n",
    "test_info.to_csv(\"output2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
